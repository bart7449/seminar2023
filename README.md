# Seminar II: Deep Learning-based Natural Language Processing
Natural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how machine understand and generate human languages.  
In recent years, deep learning approaches have obtained very high performance on many NLP tasks. 
In this course, students will gain knowledge about cutting-edge neural networks for NLP.

## Course Information
- This course meets for in-class lecture Fri 15:00 - 17:00 (Seminar room No.2 at KISTI KIUM).
- In this seminar class, students are asked to review seminal papers related to NLP techniques and present them in the class.
- Two or three papers will be reviewed every week. 
- For all inquiries related to this course, please contact kyongha@kisti.re.kr

### Instructor
This seminar is supervised by Dr. Kyong-Ha Lee at <a href="https://www.ust.ac.kr/prog/major/eng/sub03_03_02/IR/view.do?majorNo=32">KISTI campus</a>. 

### Time and Location
- Fri. 15:00  ~ 17:00
- <span style="color:red">Due to the spread of COVID 19 virus, some lectures may be held online.</span> 
## Materials
- Related lecture : Stanford CS224N http://web.stanford.edu/class/cs224n/
- A curated list of resources dedicated to Natural Language Processing https://github.com/keon/awesome-nlp
- All slides for this seminar class will be available here. 
## Logistics
- All course announcements take place though this page. Please check this page frequently.
- You must submit your presentation materials to me by e-mail one day before class.
- 
### Class components and grading
- Your grade will be recorded as a success or failure 

|Event|Date| In-class Presentation| Materials and Assignments|
|---------|----------|---------------------|------------|
|Week 1|4 March 2022| Course Introduction| None|
|Week 2|11 March 2022|<ul><li><a href="https://github.com/bart7449/seminar2022a/blob/main/NNA-Srivastava.pdf">Network archtecture(Srivastava)</a><li><a href="https://github.com/bart7449/seminar2022a/blob/main/NLPfromScratch-Sunkyong.pdf">NLP from scratch(Sunkyong)</a>|<ul><li><a href="https://cs231n.github.io/neural-networks-1/">cs231n notes on network architectures</a><li><a href="https://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf">Natural Language Processing(Almost from Scratch)</a>|
|Week 3|18 March 2022|Dismissed due to the spread of COVID-19| |
|Week 4|25 March 2022|Word embeddings</a><ul><li><a href="https://github.com/bart7449/seminar2022a/blob/main/word2vec-Juyeon.pdf">Word2Vec(Juyeon)</a><li><a href="https://github.com/bart7449/seminar2022a/blob/main/glove-Ikje.pdf">GloVe(Ikjae)</a>|<ul><li><a href="https://arxiv.org/pdf/1301.3781.pdf">Efficient Estimation of Word Representations in Vector Space</a> (original word2vec paper)<li><a href="https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf">Distributed Representations of Words and Phrases and their Compositionality</a>(negative sampling paper)|
|Week 5|1 April 2022|Embeddings for Subwords and Graphs<ul><li><a href="https://github.com/bart7449/seminar2022a/blob/main/fasttext-Srivastava.pdf">FastText(Srivastava)</a><li><a href="https://github.com/bart7449/seminar2022a/blob/main/node2vec-Tergel.pdf">node2vec:Graph embedding(Tergel)</a>|<ul><li><a href="https://watermark.silverchair.com/tacl_a_00051.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAu8wggLrBgkqhkiG9w0BBwagggLcMIIC2AIBADCCAtEGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMYfUXl8QsN1DjBzoXAgEQgIIComnopfb1mZiHWrLVWvYEkhlldmUmVWjoK5K5_3oS_Ycf24IV-x8miLywACe0hWsTFaue4DOWpPYwRm1SawmG49nI8BbCq605AofDpuHhUfvbpAuHzNIY7qMK-Ek_2GcyB_RiFN5Qe22XfUYpBpPvlMCKLkfG9JJT3bQUL_YdA6Gjc5BMbuJ5MExBPG2oUjILuTjX514xpSH6zF4VcEIQnfGzChcRvwRXA0H34NmFzHz7hY7u8lB5V7IBQm9sUKt9QM7-qcCU1guIvBAADMy9yA7LUGFqTBV7g-dimPkYPYIAEsltWgPAZVcIYHl5FGX0Glw2v87_BdK-qG5ePHly_k9OoXu7ULhQ85p7XdsSUJX4VMKOICR3g0GBTAbEIZMFThwT4foM64tYFPz4cdCvUTFU3V07IF_COM9dTM-V93FVJwVAf3p9it9U3mx6Vk3ycfYdJS2U2QqCy30tzPPTmd3sZOsb_Lvyoc9bLThclQoZcpkR5X0TsAHjF9ehxISzyrR-XCFpnlQpqM8MkcMWKRxL2aFrdLtxP-_SHaxUURtRLhvJfRs5nLZY_emBCBdH4dkK0DVZehZnjFJ4zk0QC5bvRgjt8ouhFGEfstXNLfOJGQ7UOXsRbCRKzs1C2y36b4_iwD--Mf39JJ47kwNLg5GSZRXX4tw7bz9C8a3wJ6-WFRWvkjPJqhiiJ2-874zsA_Up4dY_oAI1VgsLNg5CgnJf_YFBYAl0pOc8F1mUcHXMrq_CD1iNPijvMYmANOVM3RRZygJ-2jEkUaH9ztwIu55nVpSVvTTG_uyrRWReWm4yy1qXptxMGYdwHwiGoiLobkrO6QuCkUKdFZHudW_s_JTSqigFDYfr1CYIHFRL-r2uon4Fzx57OyusyCthEFpcxhTe">Enriching Word Vectors with Subword Information</a><li><a href="https://dl.acm.org/doi/pdf/10.1145/2939672.2939754">node2vec: Scalable Feature Learning for Networks</a>|
|Week 6|8 April 2022|<ul><li><a href="https://github.com/bart7449/seminar2022a/blob/main/ngram-sunkyong.pdf">N-gram Language Models</a>(Sunkyong)<li><a href="https://github.com/bart7449/seminar2022a/blob/main/rnn-juyeon.pdf">Sequence Modeling:Recurrent and Recursive neural Nets</a>(Juyeon)|<ul><li><a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf">N-gram Language Models (textbook chapter)</a><li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks (blog post overview)</a> <li><a href="https://www.deeplearningbook.org/contents/rnn.html">Sequence Modeling: Recurrent and Recursive Neural Nets (Sections 10.1 and 10.2)</a>|  
|Week 7|15 April 2022|<ul><li><a href="https://github.com/bart7449/seminar2022a/blob/main/recurrent-Ikjae.pdf">Recurrent modeling</a>(Ikjae)<li><a href="https://github.com/bart7449/seminar2022a/blob/main/lstm-Tergel.pdf">Vanishing Gradients and LSTM</a>(Tergel)|<ul><li><a href="https://www.deeplearningbook.org/contents/rnn.html">Sequence Modeling: Recurrent and Recursive Neural Nets (Sections 10.3, 10.5, 10.7-10.12)</a><li><a href="https://arxiv.org/pdf/1211.5063.pdf">On the difficulty of training Recurrent Neural Networks (proof of vanishing gradient problem)</a><li><a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/lectures/vanishing_grad_example.html">Vanishing Gradients Jupyter Notebook (demo for feedforward networks)</a><li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks (blog post overview)</a>|
|Week 8|22 April 2022|<ul><li><a href="https://github.com/bart7449/seminar2022a/blob/main/seq2Seq-srivastava.pdf">Machine translation</a>(Srivastava)<li><a href="https://github.com/bart7449/seminar2022a/blob/main/attention-sunkyong.pdf">Attention mechanism</a>(Sunkyong)|<ul><li><a href="https://arxiv.org/pdf/1409.3215.pdf">Sequence to Sequence Learning with Neural Networks (original seq2seq NMT paper)</a><li><a href="https://aclanthology.org/P02-1040.pdf">BLEU(original paper)</a><li><a href="https://arxiv.org/pdf/1409.0473.pdf">Neural Machine Translation by Jointly Learning to Align and Translate (original seq2seq+attention paper)</a><li><a href="https://web.stanford.edu/class/cs379c/archive/2018/class_messages_listing/content/Artificial_Neural_Network_Technology_Tutorials/OlahandCarterATTENTION-TUTORIAL-16.pdf">Attention and Augmented Recurrent Neural Networks (blog post overview)</a> |
|Week 9|29 April 2022|Transformer architecture<ul><li><a href="https://github.com/bart7449/seminar2022a/blob/main/transformer-juyeon.pdf">Transformer</a>(Juyeon)<li><a href="https://github.com/bart7449/seminar2022a/blob/main/normalization-ikjae.pdf">Layer Normalization vs Batch Normalization</a>(Ikjae)|<ul><li><a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a><li><a href="https://jalammar.github.io/illustrated-transformer/">The illustrated transformer</a><li><a href="https://arxiv.org/pdf/1607.06450.pdf">Layer Normalization</a><li><a href="http://proceedings.mlr.press/v37/ioffe15.pdf">Batch Normalization</a><li><a href="https://proceedings.neurips.cc/paper/2018/file/905056c1ac1dad141560467e0a99e1cf-Paper.pdf">How Does Batch Normalization Help Optimization?</a>| 
|Week 10|6 May 2022|<ul><li><a href="https://github.com/bart7449/seminar2022a/blob/main/bert-Tergel.pdf">BERT</a>(Tergel)<li><a href="https://github.com/bart7449/seminar2022a/blob/main/gpt3-Srivastava.pdf">Few-shot learner</a>(Srivastava) |<ul><li><a href="https://arxiv.org/pdf/1810.04805.pdf">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a><li><a href="http://jalammar.github.io/illustrated-bert/">The Illustrated BERT, ELMo, and co.</a><li><a href="https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf">Language models are few shot learners</a> |
|Week 11|13 May 2022|BERT improvements<ul><li><a href="https://github.com/bart7449/seminar2022a/blob/main/BERTimprovement-sunkyong.pdf">RoBERTa and DeBERTa</a>(Sunkyong)<li><a href="albert-juyeon.pdf">Lightweighted BERT</a>(Juyeon) |<ul><li><a href="https://arxiv.org/pdf/1907.11692.pdf%5C">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a><li><a href="https://arxiv.org/pdf/2006.03654.pdf">Deberta: Decoding-enhanced bert with disentangled attention</a><li><a href="https://arxiv.org/pdf/1909.11942.pdf?ref=https://githubhelp.com">Albert: A lite bert for self-supervised learning of language representations</a> |
|Week 12|20 May 2022| Cancellation upon requests  |  |
|Week 13|27 May 2022|<a href="https://github.com/bart7449/seminar2022a/blob/main/knowledgeLM1.pdf">Integrating knowledge in language models(I)</a>(Joint. Srivastava & Tergel) |<ul><li><a href="https://ojs.aaai.org/index.php/AAAI/article/view/5681">K-BERT: Enabling Language Representation with Knowledge Graph</a> <li><a href="https://arxiv.org/pdf/1906.07241.pdf">Barackâ€™s Wife Hillary: Using Knowledge Graphs for Fact-Aware Language Modeling</a> <li><a href="https://arxiv.org/pdf/1912.09637.pdf">Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model</a> <li><a href="https://aclanthology.org/D19-1250.pdf">Language Models as Knowledge Bases?</a>  |
|Week 14|3 June 2022|<a href="https://github.com/bart7449/seminar2022a/blob/main/knowledgeLM2.pdf">Integrating knowledge in language models(II)</a> (Joint. Ikjae & Sunkyong)|<ul><li><a href="https://arxiv.org/pdf/1909.04164.pdf">Knowledge Enhanced Contextual Word Representations</a><li><a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00360/98089">KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation</a><li><a href="https://arxiv.org/pdf/1905.07129.pdf">ERNIE: Enhanced Language Representation with Informative Entities</a><li><a href="https://arxiv.org/pdf/2010.01057.pdf">LUKE: Deep Contextualized Entity Representations with ENtity-aware self-attention</a><li><a href="https://openreview.net/pdf?id=41e9o6cQPj">GreaseLM: graph reasoning enhanced language models for question answering</a>  |
|Week 15|10 June 2022|<a href="https://github.com/bart7449/seminar2022a/blob/main/RetrievalAugmentedLM.pdf">Retreval Augmented models</a> (Joint. Juyeon & Tergel)|<ul><li><a href="https://arxiv.org/abs/2202.05262">Locating and Editing Factual Knowledge in GPT</a> <li><a href="https://arxiv.org/abs/2201.08239">LaMDA: Language Models for Dialog Applications</a> <li><a href="https://arxiv.org/abs/2002.08909">REALM: Retrieval-Augmented Language Model Pre-Training</a> <li><a href="https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a> |
|Week 16|17 June 2022|<a href="https://github.com/bart7449/seminar2022a/blob/main/pathway-ikje.pdf">Pathways</a> and <a href="https://github.com/bart7449/seminar2022a/blob/main/palm-Nilesh.pdf">PaLM</a> (Joint. Ikjae & Srivastava)|<ul><li><a href="https://arxiv.org/abs/2203.12533">Pathways: Asynchronous Distributed Dataflow for ML</a><li><a href="https://arxiv.org/pdf/2204.02311.pdf">PaLM: Scaling Language Modeling with Pathways</a>  |

  
