# Integrated Seminar
Artificial intelligence (AI) changes everything in our society and industry. In recent years, deep learning approaches have successfully achieved very high performance on many tasks.
In this course, students will acquire knowledge of cutting-edge neural network technologies in various scientific and industrial fields.

## Course Information
- This course will be held online every Thursday from 10:00 to 12:00 until December 21, 2023.
- In this seminar class, students are asked to review seminal papers related to their researches and present their research topics in the class.
- For all inquiries related to this course, please contact kyongha@kisti.re.kr

### Instructor
This seminar is jointly supervised by three UST professors 
- Dr. <a href="mailto:kyongha@kisti.re.kr">Kyong-Ha Lee</a> at <a href="https://www.ust.ac.kr/prog/major/eng/sub03_03_02/IR/view.do?majorNo=32">KISTI campus</a>.
- Dr. Hyejin Kim at <a href="https://https://www.ust.ac.kr/prog/campus/campus/sub36_04/36/majorView.do?majorNo=71&kind=information">ETRI campus</a>
- Dr. Byungjun Bae at <https://www.ust.ac.kr/prog/campus/campus/sub36_04/36/majorView.do?majorNo=70&kind=information">ETRI campus</a>

### Time and Location
- Thursday. 10:00  ~ 12:00
- ZOOM Meeting ID<span style="color:red"> 838 9307 6988</span>
- Password for entering the ZOOM meeting will be directly delievered to participants 

## Materials
- All slides for this seminar class will be available here. 
## Logistics
- All course announcements take place though this page. Please check this page frequently.
- You must submit your presentation materials to me by e-mail two days before the class time.
- 
### Class components and grading
- Your grade will be recorded as a success or failure 
- You may receive additional points for your sincere participation in Q&As.
- 
|Event|Date| In-class Presentation| Materials and Assignments|
|---------|----------|---------------------|------------|
|Week 1|4 March 2022| Course Introduction| None|
|Week 2|11 March 2022|<ul><li><a href="https://github.com/bart7449/seminar2022a/blob/main/NNA-Srivastava.pdf">Network archtecture(Srivastava)</a><li><a href="https://github.com/bart7449/seminar2022a/blob/main/NLPfromScratch-Sunkyong.pdf">NLP from scratch(Sunkyong)</a>|<ul><li><a href="https://cs231n.github.io/neural-networks-1/">cs231n notes on network architectures</a><li><a href="https://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf">Natural Language Processing(Almost from Scratch)</a>|
|Week 3|18 March 2022|Dismissed due to the spread of COVID-19| |
|Week 4|25 March 2022|Word embeddings</a><ul><li><a href="https://github.com/bart7449/seminar2022a/blob/main/word2vec-Juyeon.pdf">Word2Vec(Juyeon)</a><li><a href="https://github.com/bart7449/seminar2022a/blob/main/glove-Ikje.pdf">GloVe(Ikjae)</a>|<ul><li><a href="https://arxiv.org/pdf/1301.3781.pdf">Efficient Estimation of Word Representations in Vector Space</a> (original word2vec paper)<li><a href="https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf">Distributed Representations of Words and Phrases and their Compositionality</a>(negative sampling paper)|
|Week 5|1 April 2022|Embeddings for Subwords and Graphs<ul><li><a href="https://github.com/bart7449/seminar2022a/blob/main/fasttext-Srivastava.pdf">FastText(Srivastava)</a><li><a href="https://github.com/bart7449/seminar2022a/blob/main/node2vec-Tergel.pdf">node2vec:Graph embedding(Tergel)</a>|<ul><li><a href="https://watermark.silverchair.com/tacl_a_00051.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAu8wggLrBgkqhkiG9w0BBwagggLcMIIC2AIBADCCAtEGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMYfUXl8QsN1DjBzoXAgEQgIIComnopfb1mZiHWrLVWvYEkhlldmUmVWjoK5K5_3oS_Ycf24IV-x8miLywACe0hWsTFaue4DOWpPYwRm1SawmG49nI8BbCq605AofDpuHhUfvbpAuHzNIY7qMK-Ek_2GcyB_RiFN5Qe22XfUYpBpPvlMCKLkfG9JJT3bQUL_YdA6Gjc5BMbuJ5MExBPG2oUjILuTjX514xpSH6zF4VcEIQnfGzChcRvwRXA0H34NmFzHz7hY7u8lB5V7IBQm9sUKt9QM7-qcCU1guIvBAADMy9yA7LUGFqTBV7g-dimPkYPYIAEsltWgPAZVcIYHl5FGX0Glw2v87_BdK-qG5ePHly_k9OoXu7ULhQ85p7XdsSUJX4VMKOICR3g0GBTAbEIZMFThwT4foM64tYFPz4cdCvUTFU3V07IF_COM9dTM-V93FVJwVAf3p9it9U3mx6Vk3ycfYdJS2U2QqCy30tzPPTmd3sZOsb_Lvyoc9bLThclQoZcpkR5X0TsAHjF9ehxISzyrR-XCFpnlQpqM8MkcMWKRxL2aFrdLtxP-_SHaxUURtRLhvJfRs5nLZY_emBCBdH4dkK0DVZehZnjFJ4zk0QC5bvRgjt8ouhFGEfstXNLfOJGQ7UOXsRbCRKzs1C2y36b4_iwD--Mf39JJ47kwNLg5GSZRXX4tw7bz9C8a3wJ6-WFRWvkjPJqhiiJ2-874zsA_Up4dY_oAI1VgsLNg5CgnJf_YFBYAl0pOc8F1mUcHXMrq_CD1iNPijvMYmANOVM3RRZygJ-2jEkUaH9ztwIu55nVpSVvTTG_uyrRWReWm4yy1qXptxMGYdwHwiGoiLobkrO6QuCkUKdFZHudW_s_JTSqigFDYfr1CYIHFRL-r2uon4Fzx57OyusyCthEFpcxhTe">Enriching Word Vectors with Subword Information</a><li><a href="https://dl.acm.org/doi/pdf/10.1145/2939672.2939754">node2vec: Scalable Feature Learning for Networks</a>|
|Week 6|8 April 2022|<ul><li><a href="https://github.com/bart7449/seminar2022a/blob/main/ngram-sunkyong.pdf">N-gram Language Models</a>(Sunkyong)<li><a href="https://github.com/bart7449/seminar2022a/blob/main/rnn-juyeon.pdf">Sequence Modeling:Recurrent and Recursive neural Nets</a>(Juyeon)|<ul><li><a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf">N-gram Language Models (textbook chapter)</a><li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks (blog post overview)</a> <li><a href="https://www.deeplearningbook.org/contents/rnn.html">Sequence Modeling: Recurrent and Recursive Neural Nets (Sections 10.1 and 10.2)</a>|  
|Week 7|15 April 2022|<ul><li><a href="https://github.com/bart7449/seminar2022a/blob/main/recurrent-Ikjae.pdf">Recurrent modeling</a>(Ikjae)<li><a href="https://github.com/bart7449/seminar2022a/blob/main/lstm-Tergel.pdf">Vanishing Gradients and LSTM</a>(Tergel)|<ul><li><a href="https://www.deeplearningbook.org/contents/rnn.html">Sequence Modeling: Recurrent and Recursive Neural Nets (Sections 10.3, 10.5, 10.7-10.12)</a><li><a href="https://arxiv.org/pdf/1211.5063.pdf">On the difficulty of training Recurrent Neural Networks (proof of vanishing gradient problem)</a><li><a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/lectures/vanishing_grad_example.html">Vanishing Gradients Jupyter Notebook (demo for feedforward networks)</a><li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks (blog post overview)</a>|
|Week 8|22 April 2022|<ul><li><a href="https://github.com/bart7449/seminar2022a/blob/main/seq2Seq-srivastava.pdf">Machine translation</a>(Srivastava)<li><a href="https://github.com/bart7449/seminar2022a/blob/main/attention-sunkyong.pdf">Attention mechanism</a>(Sunkyong)|<ul><li><a href="https://arxiv.org/pdf/1409.3215.pdf">Sequence to Sequence Learning with Neural Networks (original seq2seq NMT paper)</a><li><a href="https://aclanthology.org/P02-1040.pdf">BLEU(original paper)</a><li><a href="https://arxiv.org/pdf/1409.0473.pdf">Neural Machine Translation by Jointly Learning to Align and Translate (original seq2seq+attention paper)</a><li><a href="https://web.stanford.edu/class/cs379c/archive/2018/class_messages_listing/content/Artificial_Neural_Network_Technology_Tutorials/OlahandCarterATTENTION-TUTORIAL-16.pdf">Attention and Augmented Recurrent Neural Networks (blog post overview)</a> |
|Week 9|29 April 2022|Transformer architecture<ul><li><a href="https://github.com/bart7449/seminar2022a/blob/main/transformer-juyeon.pdf">Transformer</a>(Juyeon)<li><a href="https://github.com/bart7449/seminar2022a/blob/main/normalization-ikjae.pdf">Layer Normalization vs Batch Normalization</a>(Ikjae)|<ul><li><a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a><li><a href="https://jalammar.github.io/illustrated-transformer/">The illustrated transformer</a><li><a href="https://arxiv.org/pdf/1607.06450.pdf">Layer Normalization</a><li><a href="http://proceedings.mlr.press/v37/ioffe15.pdf">Batch Normalization</a><li><a href="https://proceedings.neurips.cc/paper/2018/file/905056c1ac1dad141560467e0a99e1cf-Paper.pdf">How Does Batch Normalization Help Optimization?</a>| 
|Week 10|6 May 2022|<ul><li><a href="https://github.com/bart7449/seminar2022a/blob/main/bert-Tergel.pdf">BERT</a>(Tergel)<li><a href="https://github.com/bart7449/seminar2022a/blob/main/gpt3-Srivastava.pdf">Few-shot learner</a>(Srivastava) |<ul><li><a href="https://arxiv.org/pdf/1810.04805.pdf">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a><li><a href="http://jalammar.github.io/illustrated-bert/">The Illustrated BERT, ELMo, and co.</a><li><a href="https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf">Language models are few shot learners</a> |
|Week 11|13 May 2022|BERT improvements<ul><li><a href="https://github.com/bart7449/seminar2022a/blob/main/BERTimprovement-sunkyong.pdf">RoBERTa and DeBERTa</a>(Sunkyong)<li><a href="albert-juyeon.pdf">Lightweighted BERT</a>(Juyeon) |<ul><li><a href="https://arxiv.org/pdf/1907.11692.pdf%5C">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a><li><a href="https://arxiv.org/pdf/2006.03654.pdf">Deberta: Decoding-enhanced bert with disentangled attention</a><li><a href="https://arxiv.org/pdf/1909.11942.pdf?ref=https://githubhelp.com">Albert: A lite bert for self-supervised learning of language representations</a> |
|Week 12|20 May 2022| Cancellation upon requests  |  |
|Week 13|27 May 2022|<a href="https://github.com/bart7449/seminar2022a/blob/main/knowledgeLM1.pdf">Integrating knowledge in language models(I)</a>(Joint. Srivastava & Tergel) |<ul><li><a href="https://ojs.aaai.org/index.php/AAAI/article/view/5681">K-BERT: Enabling Language Representation with Knowledge Graph</a> <li><a href="https://arxiv.org/pdf/1906.07241.pdf">Barackâ€™s Wife Hillary: Using Knowledge Graphs for Fact-Aware Language Modeling</a> <li><a href="https://arxiv.org/pdf/1912.09637.pdf">Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model</a> <li><a href="https://aclanthology.org/D19-1250.pdf">Language Models as Knowledge Bases?</a>  |
|Week 14|3 June 2022|<a href="https://github.com/bart7449/seminar2022a/blob/main/knowledgeLM2.pdf">Integrating knowledge in language models(II)</a> (Joint. Ikjae & Sunkyong)|<ul><li><a href="https://arxiv.org/pdf/1909.04164.pdf">Knowledge Enhanced Contextual Word Representations</a><li><a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00360/98089">KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation</a><li><a href="https://arxiv.org/pdf/1905.07129.pdf">ERNIE: Enhanced Language Representation with Informative Entities</a><li><a href="https://arxiv.org/pdf/2010.01057.pdf">LUKE: Deep Contextualized Entity Representations with ENtity-aware self-attention</a><li><a href="https://openreview.net/pdf?id=41e9o6cQPj">GreaseLM: graph reasoning enhanced language models for question answering</a>  |
|Week 15|10 June 2022|<a href="https://github.com/bart7449/seminar2022a/blob/main/RetrievalAugmentedLM.pdf">Retreval Augmented models</a> (Joint. Juyeon & Tergel)|<ul><li><a href="https://arxiv.org/abs/2202.05262">Locating and Editing Factual Knowledge in GPT</a> <li><a href="https://arxiv.org/abs/2201.08239">LaMDA: Language Models for Dialog Applications</a> <li><a href="https://arxiv.org/abs/2002.08909">REALM: Retrieval-Augmented Language Model Pre-Training</a> <li><a href="https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a> |
|Week 16|17 June 2022|<a href="https://github.com/bart7449/seminar2022a/blob/main/pathway-ikje.pdf">Pathways</a> and <a href="https://github.com/bart7449/seminar2022a/blob/main/palm-Nilesh.pdf">PaLM</a> (Joint. Ikjae & Srivastava)|<ul><li><a href="https://arxiv.org/abs/2203.12533">Pathways: Asynchronous Distributed Dataflow for ML</a><li><a href="https://arxiv.org/pdf/2204.02311.pdf">PaLM: Scaling Language Modeling with Pathways</a>  |
|Week 17|17 June 2022|<ul><li>Improving Langauge Understanding by Generative Pre-Training <li>Neural Language Modeling for Contextualized Temporal Graph Generation <li>Training language models to follow instructions with human feedback <li>LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS|
  
